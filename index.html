<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>AudioSetCaps</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="AudioSetCaps" />
	<meta property="og:description" content="A 6-million Audio-Caption Paired Dataset Built with a LLMs and ALMs-based Automatic Pipeline" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">AudioSetCaps: An Enriched Audio-Caption Dataset<br> using Automated Generation Pipeline<br> with Large Audio and Language Models</span>
		
		<table align=center width=800px>
			<table align=center width=800px>
    			<br /> 
    			<br />
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://jishengbai.github.io">Jisheng Bai<sup>123*</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://haoheliu.github.io/">Haohe Liu<sup>4</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://scholar.google.com/citations?user=tNzlqKQAAAAJ">Mou Wang<sup>5</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://scholar.google.com/citations?user=oDHozOkAAAAJ&hl=zh-CN&oi=ao">Dongyuan Shi<sup>1</sup></a></span>
						</center>
					</td>
				</tr>
			</table>

			<table align=center width=800px>
				<tr>
					
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://www.surrey.ac.uk/people/wenwu-wang">Wenwu Wang<sup>4</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://www.surrey.ac.uk/people/mark-plumbley">Mark D. Plumbley<sup>4</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="https://www3.ntu.edu.sg/home/ewsgan/">Woon-Seng Gan<sup>3</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:22px"><a href="" style="color:black; text-decoration:none">Jianfeng Chen<sup>1</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
			
		  <p style="line-height:0.6" align="center">
          <sup>1</sup>School of Marine Science and Technology, Northwestern Polytechnical University, Xiâ€™an, China</font>
          </p>
          <p style="line-height:0.6" align="center">
            <a href="https://www.lfxstek.com/" style="text-decoration:none"><sup>2</sup>Xi'an Lianfeng Acoustic Technologies Co., Ltd., Xi'an, China</a></font>
          </p>
		  <p style="line-height:0.6" align="center">
          <sup>3</sup>SNTL, Nanyang Technological University, Singapore</font>
          </p>
          <p style="line-height:0.6" align="center">
          <sup>4</sup>CVSSP, University of Surrey, Guildford, UK</font>
          </p>          
          <p style="line-height:0.6" align="center">
          <sup>5</sup>Institute of Acoustics, Chinese Academy of Sciences, Beijing, China</font>
          </p>     
          
          <hr>
			<table align=center width=800px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href=''>[Arxiv]</a></span>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							<span style="font-size:24px"><a href='https://openreview.net/forum?id=uez4PMZwzP'>[NeurIPS 2024 Workshop]</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/JishengBai/AudioSetCaps'>[GitHub]</a></span><br>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href='https://huggingface.co/datasets/baijs/AudioSetCaps'>[Dataset]</a></span>
						</center>
					</td>
				</tr>
			</table>
			<hr>
		</table>
	</center>
	
    <div style="text-align: center;">
       <table style="width: 850px; margin: 0 auto;">
           <tr>
               <td style="text-align: center;">
                   <img style="width: 800px; display: block; margin: 0 auto;" src="./resources/AudioSetCaps.png" alt="AudioSetCaps Pipeline"/>
                   <p style="text-align: center;"><b>Figure 1:</b> Overview of the proposed automated pipeline for audio caption generation.</p>
               </td>
           </tr>
       </table>
    </div>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
			Building large-scale audio-language datasets is crucial yet challenging for training audio-language models, primarily due to its time-consuming and labour-intensive nature. 
            Although large language models (LLMs) have greatly enhanced the efficiency of this process, current LLM-based pipelines for generating audio-text data still lack the capability to incorporate detailed audio information.
            In this paper, we propose a novel pipeline leveraging large audio-language models to automatically generate large-scale, fine-grained audio captions.
            Based on this approach, we create AudioSetCaps, a dataset comprising 1.9 million audio-caption pairs derived from recordings in AudioSet.
            We evaluate AudioSetCaps on two downstream tasks: audio-text retrieval and automated audio captioning. 
            Models trained with AudioSetCaps achieve state-of-the-art performance on both tasks, demonstrating the high quality of the generated captions. 
            Notably, our proposed data-labelling pipeline employs open-source APIs and can run on a consumer-grade GPU. 
            To facilitate further advancements in this field, we have made our code, audio-caption paired data, and pre-trained models on downstream tasks publicly available.
			</td>
		</tr>
	</table>
	<br>
    <hr>
    
    <div style="text-align:center; font-size:22px;">
    <p>We provide the audio caption and Q&A data for the following three datasets:</p>

    <table border="1" style="border-collapse: collapse; margin: 0 auto; width: 800px;">
       <tr style="height: 50px; font-weight: bold;">
           <th style="text-align: center">Dataset</th>
           <th style="text-align: center"># Audio captions</th>
           <th style="text-align: center"># Q&A captions</th>
           <th style="text-align: center">Total</th>
       </tr>
       <tr style="height: 50px;">
           <td style="text-align: center; font-weight: bold;">AudioSetCaps</td>
           <td style="text-align: center">1910920</td>
           <td style="text-align: center">5736072</td>
           <td style="text-align: center">7646992</td>
       </tr>
       <tr style="height: 50px;">
           <td style="text-align: center; font-weight: bold;">YouTube-8M</td>
           <td style="text-align: center">4023990</td>
           <td style="text-align: center">12086037</td>
           <td style="text-align: center">16110027</td>
       </tr>
       <tr style="height: 50px;">
           <td style="text-align: center; font-weight: bold;">VGGSound</td>
           <td style="text-align: center">182189</td>
           <td style="text-align: center">592680</td>
           <td style="text-align: center">774869</td>
       </tr>
       <tr style="height: 50px;">
           <td style="text-align: center; font-weight: bold;">Total</td>
           <td style="text-align: center">6117099</td>
           <td style="text-align: center">18414789</td>
           <td style="text-align: center; font-weight: bold;">24531888</td>
       </tr>
    </table>
    </div>
    
    <br>
    <hr>
    
    <div style="text-align: center;">
       <h1>Audio-Text Retrieval</h1>
       
       <table style="width: 850px; margin: 0 auto;">
           <tr>
               <td style="text-align: center;">
                   <p style="text-align: center;">Performance comparison of audio-text retrieval on the AudioCaps test set. "LA", "AC", "WC", "ACD", and "ASC" denote LAION-Audio-630K, AudioCaps, WavCaps, Auto-ACD, and AudioSetCaps, respectively. "PT" represents pre-training on the training set and "FT" represents fine-tuning on the AudioCaps training set. "T2A" and "A2T" refer to text-to-audio and audio-to-text retrieval, respectively. "R@1" and "R@10" denote recall at ranks 1 and 10.</p>
                   <img style="width: 800px; display: block; margin: 0 auto;" src="./resources/ATR_result.png" alt="Audio-Text Retrieval Results"/>
               </td>
           </tr>
       </table>
    </div>
	
	<br>
	<hr>
	
    <div style="text-align: center;">
       <h1>Automated Audio Captioning</h1>
       
       <table style="width: 850px; margin: 0 auto;">
           <tr>
               <td style="text-align: center;">
                   <p style="text-align: center;">The performance of different methods for automated audio captioning on AudioCaps test set, where "ACT" and "CNext-Trans" refer to Audio Captioning Transformer and ConvNeXt-Transformer.</p>
                   <img style="width: 800px; display: block; margin: 0 auto;" src="./resources/AAC_result.png" alt="Automated Audio Captioning Results"/>
               </td>
           </tr>
       </table>
    </div>
	
	<br>
	<hr>
	
    <div style="text-align: center;">
       <h1>Subjective Evaluation</h1>
       
       <table style="width: 850px; margin: 0 auto;">
           <tr>
               <td style="text-align: center;">
                   <p style="text-align: center;">Mean Scores of human evaluation across datasets. The question we ask for the rater is "Please listen to the provided audio samples and rate the quality of the text annotation based on its accuracy, completeness, and presence of false information."
                   The scores indicate how well the text annotation reflects the audio content based on the following scale:
                   1-Bad, 2-Poor, 3-Fair, 4-Good, 5-Excellent.</p>
                   <img style="width: 800px; display: block; margin: 0 auto;" src="./resources/Human_scores.png" alt="Human Evaluation Scores"/>
               </td>
           </tr>
       </table>
    </div>

	<br>
	<hr>
	<br>

    <table style="width: 600px; margin: 0 auto;">
       <tr>
           <td width="200px" style="font-size: 14px;">
               <center><h1 style="font-size: 18px;">Acknowledgements</h1></center>
               <p>The training and evaluation code reference <a href="https://github.com/XinhaoMei/WavCaps"><b>WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research</b></a>.</p>
               <p>This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.</p>
           </td>
       </tr>
    </table>

<br>
</body>
</html>

